# AL Intern Coding Challenge

## Logic
I approached this problem like a graph search. It was somewhat similar to a BFS implementation, but instead of searching the neighbors for each node I would calculate possible neighbors for each node I was looking at, then quickly check if that node existed. If the node did exist, I would increment 'social_size' by 1, and add the node to a stack. Later, I would repeat the process on that node in case its neighbors were also a part of the social network. To ensure I didn't double count nodes, I kept a set of nodes that had already been viewed and checked against that. To calculate the possible neighbors, I computed the Levenshtein edits for each node.
## Efficiency
My first design choice was to make this an iterative approach rather than recursive due to the large amount of overhead python has for calling functions. Next, I changed dictionary to a set and created a set to hold visited nodes so I could look up in these datasets in constant time.
I had the idea to keep a dictionary that mapped word length to how many words in the dictionary had that length. The idea was to decrement the value every time I found a word of that length, and to check how many of those words were left before doing deletions, insertions or substitutions. If the number was small, you could do string-to-string comparisons, or if the number was zero you might not have to do any deletions, insertions, or substitutions. I tried implementing it and unfortunately found that it was slower due to the overhead of setting up the dictionary and decrementing it, and that it was never used that often.
After, it did inspire me to implement boundary conditions that would check if the word length is 1, meaning we don't need to check deletions, and if the word length was at the maximum for the dictionary, meaning we don't need to check insertions. This had a very slight speed boost.
Finally, my last idea was to try implementing some sort of list comprehension because python optimizes for it. The runtime was comparable to my other implementation, but seemed just a bit faster on average. I have included my old implementation in 'alternative.py' if you would like to see it because I think it shows a better representation of my logic.
## Test Cases
I implemented three different test suites. The first test suite focuses on small dictionaries that I thought could have strange behavior if given a wrong implementation. The second test suite is composed of the dictionaries given to us at https://gist.github.com/NWKMF/6589960bc4d6a7a22cd81feff3e0f67b. I converted the text files to json files for easier handling and loading. The files can be found in the 'resources' directory. Finally, the last test suite I implemented is to catch some random edge cases my code was failing at first. More detail can be found in the 'test.py' file. If you want to run the test docs on 'alternative.py' you will have to change 'import social_net_size as test_doc' to 'import alternative as test_doc.'
